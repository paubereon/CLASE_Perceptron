{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paubereon/CLASE_Perceptron/blob/main/Cuaderno_1_RN_El_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cuaderno 1. El perceptr贸n\n",
        "\n",
        "Un perceptr贸n es un modelo matem谩tico inspirado en las redes neuronales del cerebro humano y se utiliza en el campo del aprendizaje autom谩tico y la inteligencia artificial. Fue introducido en 1958 por Frank Rosenblatt como una forma de modelar c贸mo las neuronas procesan informaci贸n y toman decisiones.\n",
        "\n",
        "\n",
        "Concepto b谩sico:\n",
        "Un perceptr贸n es el tipo m谩s simple de red neuronal, compuesto por:\n",
        "\n",
        "**Entradas ($_1,_2,...,_$)**: Representan las caracter铆sticas del problema que queremos resolver.\n",
        "\n",
        "**Pesos ($_1,_2,...,_$):** Valores asociados a cada entrada que determinan la importancia de cada caracter铆stica.\n",
        "\n",
        "**Suma ponderada:** Se calcula como\n",
        "$=_1_1+_2_2+...+__+$, donde  es un sesgo (bias) que ajusta el modelo.\n",
        "\n",
        "**Funci贸n de activaci贸n:** Toma el resultado de la suma ponderada y produce una salida, generalmente una de dos opciones (como 0 o 1). La funci贸n de activaci贸n original en el perceptr贸n era una funci贸n escal贸n que activaba la salida si  superaba un umbral.\n",
        "\n",
        "**Funcionamiento:**\n",
        "\n",
        "El perceptr贸n se utiliza para clasificaci贸n binaria, es decir, para decidir entre dos categor铆as. Si la salida es 1, pertenece a una categor铆a; si es 0, pertenece a la otra.\n",
        "\n",
        "**Limitaciones:**\n",
        "Linealidad: Un perceptr贸n solo puede resolver problemas que sean linealmente separables, es decir, donde una l铆nea recta (o un hiperplano en dimensiones mayores) pueda dividir los datos en dos clases.\n",
        "\n",
        "No captura relaciones complejas: No puede resolver problemas como el famoso XOR, donde los datos no son linealmente separables.\n",
        "\n",
        "**Evoluci贸n:**\n",
        "El perceptr贸n es la base de las redes neuronales artificiales modernas, donde se utilizan perceptrones m煤ltiples en capas para formar redes m谩s complejas capaces de resolver problemas no lineales. Este enfoque se llama redes neuronales multicapa (o MLP, por sus siglas en ingl茅s).\n",
        "\n",
        "\n",
        "![imagen](https://github.com/adiacla/bigdata/blob/master/perceptron.png?raw=true)\n"
      ],
      "metadata": {
        "id": "-f92l4n3VGUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taller del Perceptr贸n\n",
        "\n",
        " Este cuerdno permite la introducci贸n para aprender implementar un perceptr贸n desde cero (maulmente) y luego con la biblioteca scikit-learn en Python para resolver problemas de clasificaci贸n binaria.\n",
        "\n",
        "##Datos y visualizaci贸n:\n",
        "\n",
        "Se crean datos de ejemplo sobre estudiantes con dos caracter铆sticas: notas de IA y PGA (calificiac贸n IA y el Promedio General Acumulado).\n",
        "\n",
        "Los datos se representan gr谩ficamente usando matplotlib, diferenciando entre los que se grad煤an (etiquetados como 1) y los que se retiran (etiquetados como 0).\n",
        "\n",
        "Se visualiza la dispersi贸n de los puntos con diferentes colores y formas seg煤n su clase.\n",
        "\n",
        "##Perceptr贸n b谩sico:\n",
        "\n",
        "Se implementa una funci贸n de activaci贸n (escal贸n) que toma un conjunto de pesos y calcula la salida en funci贸n de la entrada.\n",
        "La salida es 1 si la combinaci贸n ponderada de las entradas supera un umbral, y 0 en caso contrario.\n",
        "Luego, se inicializan aleatoriamente los pesos y el sesgo y se aplica la activaci贸n para ver c贸mo se comporta el modelo con entradas dadas.\n",
        "Entrenamiento del Perceptr贸n:\n",
        "\n",
        "El perceptr贸n se entrena iterativamente en un ciclo de 茅pocas. Durante cada 茅poca, el modelo ajusta los pesos en funci贸n del error de predicci贸n (diferencia entre el valor real y el predicho).\n",
        "\n",
        "El error total se calcula y se ajustan los pesos utilizando una tasa de aprendizaje (learning rate)\n",
        ".\n",
        "Este proceso se repite hasta que el error se minimice.\n",
        "\n",
        "##Implementaci贸n con scikit-learn:\n",
        "\n",
        "Se utiliza el Perceptron de scikit-learn para entrenar el modelo con las mismas entradas. Este modelo implementa una versi贸n optimizada y f谩cil de usar.\n",
        "\n",
        "Se ajustan los par谩metros como la tasa de aprendizaje, el n煤mero m谩ximo de iteraciones (max_iter), y el criterio de convergencia utilizando tol (tolerancia).\n",
        "\n",
        "Se entrenan los datos y se eval煤a el modelo con predicciones sobre nuevos puntos.\n",
        "\n",
        "Tambi茅n se obtiene el coeficiente y el sesgo del modelo entrenado, as铆 como el n煤mero de iteraciones que utiliz贸.\n",
        "\n",
        "##Escalado de datos y modelos en conjunto con scikit-learn:\n",
        "\n",
        "Antes de entrenar un modelo con scikit-learn, se escalaban los datos usando StandardScaler para normalizar las caracter铆sticas y mejorar el rendimiento del modelo.\n",
        "Se divide el conjunto de datos en entrenamiento y prueba usando train_test_split.\n",
        "\n",
        "##Evaluaci贸n del modelo:\n",
        "\n",
        "Se evalu贸 el modelo utilizando m茅tricas como la precisi贸n (accuracy), la matriz de confusi贸n (para ver los aciertos y errores de clasificaci贸n), y el informe de clasificaci贸n.\n",
        "\n",
        "Se muestra la curva ROC y se calcula el rea Bajo la Curva (AUC) como una medida adicional de la calidad del modelo.\n",
        "\n",
        "#Uso de t茅cnicas avanzadas:\n",
        "\n",
        "Se menciona el uso de t茅cnicas como early stopping, que detiene el entrenamiento del modelo cuando ya no se observa mejora en la precisi贸n.\n",
        "\n",
        "Tambi茅n se implementan evaluaciones comparativas entre precisi贸n y AUC, explicando c贸mo cada una puede ofrecer diferentes perspectivas sobre el rendimiento del modelo.\n",
        "\n",
        "Este c贸digo cubre un ciclo completo de trabajo con perceptrones, desde la implementaci贸n m谩s b谩sica hasta el uso de modelos optimizados y t茅cnicas avanzadas en scikit-learn. Es un excelente ejercicio para entender c贸mo funcionan los perceptrones y c贸mo se pueden evaluar y ajustar los modelos de clasificaci贸n binaria.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BAmn_bbY_-qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importar librerias y Preparaci贸n de datos"
      ],
      "metadata": {
        "id": "4Bmj-fsUXu52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "CoshbDo0YAeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIg7eGg2_5fm"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 1. Preparaci贸n de los Datos\n",
        "# ======================\n",
        "# Datos de estudiantes representados como [notas_IA, PGA], con valores normalizados entre 0 y 1.\n",
        "alumnos = np.array([[0.3, 0.4], [0.4, 0.3],\n",
        "                     [0.3, 0.2], [0.4, 0.1],\n",
        "                     [0.5, 0.2], [0.4, 0.8],\n",
        "                     [0.6, 0.8], [0.5, 0.6],\n",
        "                     [0.7, 0.6], [0.8, 0.5]])\n",
        "\n",
        "# Etiquetas de clase: 1 = Se grad煤a, 0 = Se retira\n",
        "clases = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combinar los datos y las clases para facilitar la visualizaci贸n\n",
        "data = np.column_stack((alumnos, clases))\n",
        "\n",
        "# Crear un DataFrame para an谩lisis\n",
        "columnas = ['Nota IA', 'GPA', 'Estado']\n",
        "df = pd.DataFrame(data, columns=columnas)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "--R1eN9X66M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3aT2r2CJq_KK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Verificar si son linealmente separables\n",
        "\n",
        "En la gr谩fica puede apreciarse si a trav茅s de una l铆nea recta se pueden separar los estudiantes que se grad煤an o se retiran"
      ],
      "metadata": {
        "id": "gWUfwzMtYS0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ======================\n",
        "# 2. Visualizaci贸n de Datos\n",
        "# ======================\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.title(\"驴xito en Ingenier铆a?\", fontsize=20)\n",
        "plt.scatter(alumnos[clases == 0].T[0],\n",
        "            alumnos[clases == 0].T[1],\n",
        "            marker=\"x\", s=180, color=\"red\",\n",
        "            linewidths=5, label=\"Se retira\")\n",
        "plt.scatter(alumnos[clases == 1].T[0],\n",
        "            alumnos[clases == 1].T[1],\n",
        "            marker=\"o\", s=180, color=\"blue\",\n",
        "            linewidths=5, label=\"Se grad煤a\")\n",
        "plt.xlabel(\"Nota IA\", fontsize=15)\n",
        "plt.ylabel(\"PGA\", fontsize=15)\n",
        "plt.legend(bbox_to_anchor=(1.3, 0.15))\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SUDZSWh7AlvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementaci贸n del perceptr贸n manualmente\n",
        "##Definici贸n de la funci贸n de activaci贸n"
      ],
      "metadata": {
        "id": "zPwwUzknA_Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 3. Implementaci贸n del Perceptr贸n\n",
        "# ======================\n",
        "# Funci贸n de activaci贸n (Escal贸n): Decide la clase bas谩ndose en la suma ponderada\n",
        "\n",
        "def activacion(pesos, x, b):\n",
        "    z = np.dot(pesos, x)\n",
        "    return 1 if z + b > 0 else 0\n",
        "\n",
        "#Recordar que z es la funci贸n suma y b es el bias."
      ],
      "metadata": {
        "id": "QM8PotpCBBR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align}\n",
        "         & z= w_1 x_1 + w_2 x_2+b\n",
        "\\end{align}\n",
        "\n",
        "![imagen](https://github.com/adiacla/bigdata/blob/master/FuncionEscalon.png?raw=true)"
      ],
      "metadata": {
        "id": "dMvwWaIh_TqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer una semilla para reproducibilidad\n",
        "np.random.seed(42)  # Puedes cambiar 42 por cualquier n煤mero entero\n",
        "\n",
        "# Inicializar pesos y bias aleatorios\n",
        "pesos = np.random.uniform(-1, 1, size=2)\n",
        "b = np.random.uniform(-1, 1)\n",
        "print(\"Pesos W1:\", pesos[0], \"   Pesos W2:\", pesos[1])\n",
        "print(\"Bias o umbral:\", b)\n",
        "#El aleatorio puede variar si cambia la semilla."
      ],
      "metadata": {
        "id": "XtS2k1mc-Dqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![imagen](https://github.com/adiacla/bigdata/blob/master/pesosybias.png?raw=true)"
      ],
      "metadata": {
        "id": "4UQ9qj5FuXuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predicciones de pruebas\n",
        "tenemos dos alumnos nuevos que tienen estos datosen las notas de IA y GPA (calificiac贸n IA y el Promedio General Acumulado)\n",
        "\n",
        "a. [0.4, 0.3]\n",
        "b. [0.8, 0.9]\n",
        "\n",
        "Hagamos predicciones"
      ],
      "metadata": {
        "id": "Y4tJPFK2vFe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar la activaci贸n con datos de ejemplo\n",
        "print(f\"Predicci贸n para [0.4, 0.3]: {activacion(pesos, [0.4, 0.3], b)}\")\n",
        "print(f\"Predicci贸n para [0.8, 0.9]: {activacion(pesos, [0.8, 0.9], b)}\")"
      ],
      "metadata": {
        "id": "5Ltrk6ThC8dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asi acabamos de hacer un peque帽o perceptr贸n, pero es claro que el valor de w1,w2 y b fueron aleatorios y no est谩n optimizados, ahora debemos entrenar.\n",
        " por lo tanto la predicci贸n **es errada.**\n",
        "\n",
        "\n",
        "Vamos a optimizar a partir del error de activaci贸n."
      ],
      "metadata": {
        "id": "471G3dUqCxMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pseudoc贸digo del entrenamiento del Perceptr贸n\n",
        "\n",
        "Haremos un ciclo para ir cambiando los datos de los pesos y el umbral, hasta tener el menor error aceptable (por ejemplo si es cero) o se terminen las epocas.\n",
        "\n",
        "La tasa de aprendizaje es la tasa con la que se va actualizar los pesos, para que se vayan ajustando hasta que lleguen a valor m铆nimo de error, debemos probar con varias tasas.\n",
        "\n",
        "##Algortimo\n",
        "\n",
        "0. Inicializar los pesos y el umbral\n",
        "1. inicia epocas m谩ximas = 100 (Se puede modificar)\n",
        "2. inicia epoca = 0\n",
        "3. inicia tasa de aprendizaje = 0.01 (Se puede modificar)\n",
        "4. Mientras (epoca < epocas m谩ximas) hacer:\n",
        "5.    Para cada instancia (o registro) de entrenamiento hacer:\n",
        "          Calcula salida del perceptr贸n para esa estancia\n",
        "          Calcula el error\n",
        "          Actualiza pesos y umbral usando la tasa de aprendizaje, la instancia y el error\n",
        "9.    epoca += 1"
      ],
      "metadata": {
        "id": "iuwIq-ijBHTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 4. Entrenamiento del Perceptr贸n\n",
        "# ======================\n",
        "# Inicializaci贸n de par谩metros\n",
        "pesos = np.random.uniform(-1, 1, size=2)\n",
        "b = np.random.uniform(-1, 1)\n",
        "epocas = 100\n",
        "tasa_de_aprendizaje = 0.01\n",
        "\n",
        "# Ciclo de entrenamiento\n",
        "for epoca in range(epocas):\n",
        "    error_total = 0\n",
        "\n",
        "    for i in range(len(alumnos)):\n",
        "        # Realizar una predicci贸n\n",
        "        prediccion = activacion(pesos, alumnos[i], b)\n",
        "\n",
        "        # Calcular el error\n",
        "        error = clases[i] - prediccion\n",
        "        error_total += error ** 2\n",
        "\n",
        "        # Actualizar los pesos y el bias\n",
        "        pesos += tasa_de_aprendizaje * error * alumnos[i]\n",
        "        b += tasa_de_aprendizaje * error\n",
        "\n",
        "    # Imprimir el error total por 茅poca\n",
        "    print(f\"poca {epoca + 1}: Error total = {error_total}\")\n"
      ],
      "metadata": {
        "id": "Q3IsMHTxBMCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C贸mo se puede obervar desde la 茅poca 65 ya no tenemos errores, por lo tanto el modelo \"convergio\" a 0 errores en total a partir de ajustar los pesos y los bias."
      ],
      "metadata": {
        "id": "1vgp95X5w0ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##poca en el Entrenamiento de un Modelo\n",
        "En el contexto del aprendizaje autom谩tico, una 茅poca se refiere a una iteraci贸n completa sobre el conjunto de datos de entrenamiento. Durante una 茅poca, el modelo:\n",
        "\n",
        "* Toma cada instancia del conjunto de datos de entrenamiento.\n",
        "\n",
        "* Calcula predicciones utilizando la funci贸n de activaci贸n.\n",
        "\n",
        "* Actualiza los pesos y el umbral (o bias) en funci贸n del error cometido.\n",
        "\n",
        "Un modelo puede necesitar varias 茅pocas para que sus par谩metros (pesos y umbral) se ajusten adecuadamente y minimicen el error de predicci贸n.\n",
        "\n",
        "##Cantidad de 茅pocas necesarias\n",
        "\n",
        "**Pocas 茅pocas:** El modelo puede no aprender lo suficiente, resultando en un modelo subentrenado (underfitting).\n",
        "\n",
        "**Demasiadas 茅pocas:** El modelo puede aprender detalles espec铆ficos del conjunto de datos, lo que lleva a sobreentrenamiento (overfitting).\n",
        "\n",
        "\n",
        "***Regla general:*** Comienza con un n煤mero moderado, como 100 o 500, y eval煤a el desempe帽o. Si el error sigue disminuyendo, podr铆as aumentar las 茅pocas.\n",
        "\n"
      ],
      "metadata": {
        "id": "C6f9N0thyugw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tasa de Aprendizaje\n",
        "La tasa de aprendizaje () es un par谩metro que controla la magnitud de los ajustes realizados a los pesos y al umbral despu茅s de cada predicci贸n.\n",
        "\n",
        "Es un n煤mero peque帽o, t铆picamente en el rango de 0.001  a\n",
        "0.1.\n",
        "Determina qu茅 tan r谩pido o lento aprende el modelo.\n",
        "\n",
        "F贸rmula de actualizaci贸n de los pesos:\n",
        "$$_{ｐ=ゐ○}+errorentrada$$\n",
        "\n",
        "**Impacto del valor de la tasa de aprendizaje:**Tasa de aprendizaje muy alta (=0.5 o m谩s):\n",
        "\n",
        "El modelo puede ajustar demasiado r谩pido, dando lugar a oscilaciones alrededor de los valores ideales.\n",
        "\n",
        "Podr铆a no converger a una soluci贸n estable.\n",
        "\n",
        "**Tasa de aprendizaje muy baja** (<=0.0001):\n",
        "\n",
        "El modelo aprender谩 muy lentamente.\n",
        "Puede necesitar muchas 茅pocas para alcanzar un error aceptable.\n",
        "\n",
        "\n",
        "***Regla general:** Comienza con una tasa de aprendizaje entre\n",
        "0.01 y 0.1.\n",
        "\n",
        "***Ajusta seg煤n el comportamiento del modelo:**\n",
        "\n",
        "Si el error no disminuye lo suficiente, incrementa ligeramente.\n",
        "\n",
        "Si el error oscila o no converge, reduce el valor.\n",
        "\n",
        "\n",
        "**Visualizaci贸n del error:**\n",
        "\n",
        "Traza una gr谩fica del error total frente al n煤mero de 茅pocas.\n",
        "Si el error disminuye de manera consistente y se estabiliza, los valores pueden ser adecuados.\n",
        "\n",
        "Si el error oscila o se detiene prematuramente, ajusta la tasa de aprendizaje o incrementa las 茅pocas.\n",
        "![imagen](https://github.com/adiacla/bigdata/blob/master/epochs.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "**Validaci贸n cruzada:**\n",
        "\n",
        "* Divide los datos en entrenamiento y validaci贸n.\n",
        "* Ajusta la tasa de aprendizaje y el n煤mero de 茅pocas para minimizar el error en el conjunto de validaci贸n, evitando el sobreentrenamiento.\n",
        "\n",
        "**Uso de t茅cnicas avanzadas:**\n",
        "\n",
        "Implementa un programa que disminuya autom谩ticamente la tasa de aprendizaje durante el entrenamiento si el modelo no mejora (decay learning rate).\n",
        "\n",
        "Usa herramientas como grid search o random search para encontrar los mejores hiperpar谩metros.\n",
        "\n",
        "**Analog铆a para Facilitar la Comprensi贸n**\n",
        "\n",
        "Piensa en la tasa de aprendizaje como el tama帽o de los pasos de una persona y en las 茅pocas como el n煤mero total de pasos en una caminata:\n",
        "\n",
        "* Tasa de aprendizaje grande: Das zancadas largas, pero podr铆as pasar de largo el destino.\n",
        "* Tasa de aprendizaje peque帽a: Das pasos cortos, pero llegar谩s al destino m谩s lentamente.\n",
        "\n",
        "pocas: Representan cu谩ntos pasos est谩s dispuesto a dar para llegar a tu objetivo.\n",
        "\n",
        "El equilibrio ideal depende del terreno (tus datos) y la distancia al destino (minimizar el error).\n",
        "\n"
      ],
      "metadata": {
        "id": "Njo1Mdj_znWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predecir el con perceptr贸n entrenado\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-EriWj9Xw-by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar la activaci贸n con datos de ejemplo\n",
        "print(f\"Predicci贸n para [0.4, 0.3]: {activacion(pesos, [0.4, 0.3], b)}\")\n",
        "print(f\"Predicci贸n para [0.8, 0.9]: {activacion(pesos, [0.8, 0.9], b)}\")"
      ],
      "metadata": {
        "id": "0eUcGMyt7A2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En conclusi贸n vemos c贸mo ahora la predicci贸n para el alumno con las notas: [0.4, 0.3] es 0 es decir que se retira. Contratio al alumno [0.8, 0.9]: 1, que se gradua."
      ],
      "metadata": {
        "id": "sxaia6cTxc22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graficar las Zonas de aprobaci贸n\n",
        "\n",
        "En la siguiete gr谩fica vemos c贸mo el modelo separa los que se graduan y los que se retirar, en una gr谩fica 2D, dados que es f谩cil apreciarlos porque tenemos solo dos carcteristicas (Notas IA y PGA). En el fondo de la gr谩fica notar谩 unos puntos azules y otros rojos. Esas son las zonas de separaci贸n."
      ],
      "metadata": {
        "id": "JCcvObQIBT4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 5. Visualizaci贸n de la Clasificaci贸n\n",
        "# ======================\n",
        "plt.figure(figsize=(6, 5), dpi=200)\n",
        "plt.title(\"驴xito en Ingenier铆a?\", fontsize=20)\n",
        "\n",
        "# Graficar los puntos de datos\n",
        "plt.scatter(alumnos[clases == 0].T[0],\n",
        "            alumnos[clases == 0].T[1],\n",
        "            marker=\"x\", s=180, color=\"red\",\n",
        "            linewidths=5, label=\"Se retira\")\n",
        "plt.scatter(alumnos[clases == 1].T[0],\n",
        "            alumnos[clases == 1].T[1],\n",
        "            marker=\"o\", s=180, color=\"blue\",\n",
        "            linewidths=5, label=\"Se grad煤a\")\n",
        "\n",
        "# Graficar las regiones de decisi贸n\n",
        "for nota in np.arange(0, 1, 0.05):\n",
        "    for GPA in np.arange(0, 1, 0.05):\n",
        "        color = activacion(pesos, [nota, GPA], b)\n",
        "        plt.scatter(nota, GPA, marker=\"s\", s=10,\n",
        "                    color=\"blue\" if color == 1 else \"red\",\n",
        "                    alpha=0.2, linewidths=0)\n",
        "\n",
        "plt.xlabel(\"Nota IA\", fontsize=15)\n",
        "plt.ylabel(\"PGA\", fontsize=15)\n",
        "plt.legend(bbox_to_anchor=(1.3, 0.15))\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xvnlPrZMBEj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptr贸n con Scikit-learn\n",
        "\n",
        "El objetivo es replicar la l贸gica de un perceptr贸n manual, pero utilizando la implementaci贸n optimizada que proporciona la biblioteca scikit-learn. Esto nos permitir谩 realizar tareas similares con menos esfuerzo, mayor optimizaci贸n, y acceso a m茅tricas adicionales.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jagm6BoIDJeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicaci贸n de los pasos\n",
        "1. Importaci贸n de bibliotecas\n",
        "Primero, importamos las bibliotecas necesarias:\n",
        "\n",
        "**Perceptron de sklearn.linear_model:** Clase que implementa el algoritmo del perceptr贸n.\n",
        "\n",
        "**numpy:** Para trabajar con los datos en forma de matrices.\n",
        "\n"
      ],
      "metadata": {
        "id": "oHln7SyY42QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7MGMP8n55B4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Datos de entrenamiento\n",
        "Usamos los mismos datos de entrenamiento del ejemplo anterior. Estos datos contienen:\n",
        "\n",
        "X_train: Las caracter铆sticas de los alumnos (notas_IA y PGA).\n",
        "y_train: Las etiquetas de clase (0: \"Se retira\", 1: \"Se grad煤a\")."
      ],
      "metadata": {
        "id": "ddBj3bA75Fp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos de entrenamiento\n",
        "X_train = alumnos  # Caracter铆sticas\n",
        "y_train = clases   # Etiquetas"
      ],
      "metadata": {
        "id": "-xqyfjlA5Qom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creaci贸n del modelo\n",
        "Creamos una instancia de la clase Perceptron.\n",
        "\n",
        "Algunos par谩metros importantes:\n",
        "\n",
        "verbose=True: Muestra informaci贸n sobre el progreso del entrenamiento.\n",
        "\n",
        "Otros par谩metros opcionales (no utilizados aqu铆):\n",
        "\n",
        "max_iter: N煤mero m谩ximo de 茅pocas. Por defecto,\n",
        "1000.\n",
        "\n",
        "eta0: Tasa de aprendizaje inicial. Por defecto,\n",
        "1.0"
      ],
      "metadata": {
        "id": "3JkgkQ_D5TXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo de Perceptr贸n\n",
        "perceptron = Perceptron(verbose=True)"
      ],
      "metadata": {
        "id": "kwjWaxqz5gAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Entrenamiento del modelo\n",
        "\n",
        "El m茅todo fit ajusta el modelo al conjunto de datos de entrenamiento. Internamente:\n",
        "\n",
        "* Inicializa pesos y bias.\n",
        "\n",
        "* Itera por los datos, ajustando los pesos en funci贸n del error."
      ],
      "metadata": {
        "id": "GrKGmiEm5nXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo\n",
        "perceptron.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "0g4PLTrE5vCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Realizaci贸n de predicciones\n",
        "\n",
        "El m茅todo predict se usa para predecir la clase de nuevos datos. Aqu铆 realizamos predicciones para dos instancias nuevas:\n",
        "\n",
        "[0.2,0.2]: Caracter铆sticas bajas.\n",
        "\n",
        "[0.8,0.8]: Caracter铆sticas altas"
      ],
      "metadata": {
        "id": "e9Zp_uGe5979"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones con datos nuevos\n",
        "predictions = perceptron.predict([[0.2, 0.2], [0.8, 0.8]])\n",
        "print(\"Predicciones:\", predictions)\n"
      ],
      "metadata": {
        "id": "REm_A_Mp6Gub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inspecci贸n del modelo entrenado\n",
        "\n",
        "Despu茅s del entrenamiento, se pueden consultar varias propiedades del modelo:\n",
        "\n",
        "* n_iter_: N煤mero de iteraciones realizadas (茅pocas).\n",
        "coef_: Pesos aprendidos (1 ,2, etc.).\n",
        "\n",
        "* intercept_: El bias o umbral.\n",
        "\n",
        "* n_features_in_: N煤mero de caracter铆sticas usadas en el entrenamiento.\n"
      ],
      "metadata": {
        "id": "2KceudO66L0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron"
      ],
      "metadata": {
        "id": "10sJdpl7VCa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consultar informaci贸n del modelo\n",
        "print(\"pocas realizadas:\", perceptron.n_iter_)\n",
        "print(\"Pesos aprendidos:\", perceptron.coef_)\n",
        "print(\"El bias es:\", perceptron.intercept_)\n",
        "print(\"N煤mero de caracter铆sticas utilizadas:\", perceptron.n_features_in_)\n"
      ],
      "metadata": {
        "id": "8mL1RjiCVez1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#permite saber las clases que predice\n",
        "perceptron.classes_"
      ],
      "metadata": {
        "id": "AFN_9ecfVkKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ajustando par谩metros\n",
        "\n",
        "1. learning_rate (Tasa de aprendizaje)\n",
        "\n",
        "En el Perceptr贸n de sklearn, no configuramos directamente el learning rate, pero se controla impl铆citamente a trav茅s de la tasa inicial (0 ) y del algoritmo de optimizaci贸n.\n",
        "\n",
        "En el perceptr贸n est谩ndar, esta tasa define cu谩nto se ajustan los pesos en cada iteraci贸n en funci贸n del error.\n",
        "\n",
        "2. max_iter (N煤mero m谩ximo de iteraciones o 茅pocas)\n",
        "\n",
        "Especifica el n煤mero m谩ximo de iteraciones sobre los datos de entrenamiento. Si el modelo converge antes, el entrenamiento puede detenerse prematuramente.\n",
        "\n",
        "**Ajustar este par谩metro es clave:**\n",
        "\n",
        "* Valores bajos: Entrenamiento r谩pido pero puede no alcanzar la convergencia.\n",
        "* Valores altos: Entrenamiento m谩s largo con mayores posibilidades de convergencia.\n",
        "\n",
        "3. tol (Tolerancia)\n",
        "\n",
        "Es el valor l铆mite que define si el algoritmo ha convergido.\n",
        "Cuando la reducci贸n del error entre 茅pocas sucesivas es menor que este valor, el entrenamiento se detiene.\n",
        "\n",
        "**Valores comunes:**\n",
        "\n",
        "* tol=1e-3 (0.001): Tolerancia est谩ndar.\n",
        "* Valores menores (ej., 1e-5) permiten m谩s precisi贸n pero aumentan el tiempo de entrenamiento.\n",
        "\n",
        "4. random_state (Semilla aleatoria)\n",
        "\n",
        "Determina el estado inicial del generador de n煤meros aleatorios utilizado para inicializar pesos.\n",
        "\n",
        "**Es 煤til para:**\n",
        "\n",
        "* Reproducibilidad: Obtener los mismos resultados en ejecuciones diferentes.\n",
        "* Pruebas controladas: Comparar el desempe帽o con diferentes configuraciones."
      ],
      "metadata": {
        "id": "7MCZqkWL64kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Early Stopping</h3>\n",
        "<ul>\n",
        "<li><strong>Definici&oacute;n</strong>: El <code>early stopping</code> es una t&eacute;cnica que detiene el entrenamiento de un modelo cuando el rendimiento en un conjunto de validaci&oacute;n comienza a empeorar. Esto se hace para prevenir el sobreajuste, es decir, cuando el modelo empieza a aprender el ruido en lugar de las verdaderas relaciones en los datos.</li>\n",
        "<li><strong>Uso</strong>: Se suele utilizar en el entrenamiento de redes neuronales y otros modelos que se entrenan durante muchas &eacute;pocas. Se monitorea el rendimiento en un conjunto de validaci&oacute;n y se detiene el entrenamiento si no hay mejora durante un n&uacute;mero espec&iacute;fico de &eacute;pocas (llamado \"patience\").</li>\n",
        "<li><strong>Objetivo</strong>: Asegurarse de que el modelo generalice bien a datos no vistos, evitando que se sobreajuste a los datos de entrenamiento.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "e3hEV9YR8wYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(clases),len(alumnos)"
      ],
      "metadata": {
        "id": "EO6BPjb05iZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "import numpy as np\n",
        "\n",
        "# Datos de entrenamiento\n",
        "X_train = alumnos  # Caracter铆sticas: [notas_IA, PGA]\n",
        "y_train = clases   # Etiquetas: [0 (Se retira), 1 (Se grad煤a)]\n",
        "\n",
        "# Crear el modelo del Perceptr贸n\n",
        "# max_iter=500: Permitir hasta 500 iteraciones (茅pocas)\n",
        "# tol=1e-3: Detener si el cambio en el error es menor a 0.001\n",
        "# verbose=1: Mostrar informaci贸n del entrenamiento\n",
        "# random_state=123: Fijar la semilla para reproducibilidad\n",
        "perceptron = Perceptron(max_iter=500, tol=1e-3, verbose=1, random_state=123,early_stopping=False)\n",
        "\n",
        "# Entrenar el modelo\n",
        "perceptron.fit(X_train, y_train)\n",
        "print(\"-\"*80,\"\\n\")\n",
        "# Hacer predicciones\n",
        "# Ejemplo: Predecir las clases para las caracter铆sticas [0.2, 0.2] y [0.8, 0.8]\n",
        "predictions = perceptron.predict([[0.2, 0.2], [0.8, 0.8]])\n",
        "print(\"Predicciones:\", predictions)\n",
        "\n",
        "print(\"-\"*80,\"\\n\")\n",
        "# Inspeccionar el modelo entrenado\n",
        "print(\"Pesos aprendidos (coeficientes):\", perceptron.coef_)\n",
        "print(\"Bias aprendido:\", perceptron.intercept_)\n",
        "print(\"N煤mero de iteraciones realizadas:\", perceptron.n_iter_)\n"
      ],
      "metadata": {
        "id": "182cB2bvWG9r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paubereon/CLASE_Perceptron/blob/main/Cuaderno_1_RN_El_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cuaderno 1. El perceptrón\n",
        "\n",
        "Un perceptrón es un modelo matemático inspirado en las redes neuronales del cerebro humano y se utiliza en el campo del aprendizaje automático y la inteligencia artificial. Fue introducido en 1958 por Frank Rosenblatt como una forma de modelar cómo las neuronas procesan información y toman decisiones.\n",
        "\n",
        "\n",
        "Concepto básico:\n",
        "Un perceptrón es el tipo más simple de red neuronal, compuesto por:\n",
        "\n",
        "**Entradas ($𝑥_1,𝑥_2,...,𝑥_𝑛$)**: Representan las características del problema que queremos resolver.\n",
        "\n",
        "**Pesos ($𝑤_1,𝑤_2,...,𝑤_𝑛$):** Valores asociados a cada entrada que determinan la importancia de cada característica.\n",
        "\n",
        "**Suma ponderada:** Se calcula como\n",
        "$𝑧=𝑤_1𝑥_1+𝑤_2𝑥_2+...+𝑤_𝑛𝑥_𝑛+𝑏$, donde 𝑏 es un sesgo (bias) que ajusta el modelo.\n",
        "\n",
        "**Función de activación:** Toma el resultado de la suma ponderada y produce una salida, generalmente una de dos opciones (como 0 o 1). La función de activación original en el perceptrón era una función escalón que activaba la salida si 𝑧 superaba un umbral.\n",
        "\n",
        "**Funcionamiento:**\n",
        "\n",
        "El perceptrón se utiliza para clasificación binaria, es decir, para decidir entre dos categorías. Si la salida es 1, pertenece a una categoría; si es 0, pertenece a la otra.\n",
        "\n",
        "**Limitaciones:**\n",
        "Linealidad: Un perceptrón solo puede resolver problemas que sean linealmente separables, es decir, donde una línea recta (o un hiperplano en dimensiones mayores) pueda dividir los datos en dos clases.\n",
        "\n",
        "No captura relaciones complejas: No puede resolver problemas como el famoso XOR, donde los datos no son linealmente separables.\n",
        "\n",
        "**Evolución:**\n",
        "El perceptrón es la base de las redes neuronales artificiales modernas, donde se utilizan perceptrones múltiples en capas para formar redes más complejas capaces de resolver problemas no lineales. Este enfoque se llama redes neuronales multicapa (o MLP, por sus siglas en inglés).\n",
        "\n",
        "\n",
        "![imagen](https://github.com/adiacla/bigdata/blob/master/perceptron.png?raw=true)\n"
      ],
      "metadata": {
        "id": "-f92l4n3VGUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taller del Perceptrón\n",
        "\n",
        " Este cuerdno permite la introducción para aprender implementar un perceptrón desde cero (maulmente) y luego con la biblioteca scikit-learn en Python para resolver problemas de clasificación binaria.\n",
        "\n",
        "##Datos y visualización:\n",
        "\n",
        "Se crean datos de ejemplo sobre estudiantes con dos características: notas de IA y PGA (calificiacón IA y el Promedio General Acumulado).\n",
        "\n",
        "Los datos se representan gráficamente usando matplotlib, diferenciando entre los que se gradúan (etiquetados como 1) y los que se retiran (etiquetados como 0).\n",
        "\n",
        "Se visualiza la dispersión de los puntos con diferentes colores y formas según su clase.\n",
        "\n",
        "##Perceptrón básico:\n",
        "\n",
        "Se implementa una función de activación (escalón) que toma un conjunto de pesos y calcula la salida en función de la entrada.\n",
        "La salida es 1 si la combinación ponderada de las entradas supera un umbral, y 0 en caso contrario.\n",
        "Luego, se inicializan aleatoriamente los pesos y el sesgo y se aplica la activación para ver cómo se comporta el modelo con entradas dadas.\n",
        "Entrenamiento del Perceptrón:\n",
        "\n",
        "El perceptrón se entrena iterativamente en un ciclo de épocas. Durante cada época, el modelo ajusta los pesos en función del error de predicción (diferencia entre el valor real y el predicho).\n",
        "\n",
        "El error total se calcula y se ajustan los pesos utilizando una tasa de aprendizaje (learning rate)\n",
        ".\n",
        "Este proceso se repite hasta que el error se minimice.\n",
        "\n",
        "##Implementación con scikit-learn:\n",
        "\n",
        "Se utiliza el Perceptron de scikit-learn para entrenar el modelo con las mismas entradas. Este modelo implementa una versión optimizada y fácil de usar.\n",
        "\n",
        "Se ajustan los parámetros como la tasa de aprendizaje, el número máximo de iteraciones (max_iter), y el criterio de convergencia utilizando tol (tolerancia).\n",
        "\n",
        "Se entrenan los datos y se evalúa el modelo con predicciones sobre nuevos puntos.\n",
        "\n",
        "También se obtiene el coeficiente y el sesgo del modelo entrenado, así como el número de iteraciones que utilizó.\n",
        "\n",
        "##Escalado de datos y modelos en conjunto con scikit-learn:\n",
        "\n",
        "Antes de entrenar un modelo con scikit-learn, se escalaban los datos usando StandardScaler para normalizar las características y mejorar el rendimiento del modelo.\n",
        "Se divide el conjunto de datos en entrenamiento y prueba usando train_test_split.\n",
        "\n",
        "##Evaluación del modelo:\n",
        "\n",
        "Se evaluó el modelo utilizando métricas como la precisión (accuracy), la matriz de confusión (para ver los aciertos y errores de clasificación), y el informe de clasificación.\n",
        "\n",
        "Se muestra la curva ROC y se calcula el Área Bajo la Curva (AUC) como una medida adicional de la calidad del modelo.\n",
        "\n",
        "#Uso de técnicas avanzadas:\n",
        "\n",
        "Se menciona el uso de técnicas como early stopping, que detiene el entrenamiento del modelo cuando ya no se observa mejora en la precisión.\n",
        "\n",
        "También se implementan evaluaciones comparativas entre precisión y AUC, explicando cómo cada una puede ofrecer diferentes perspectivas sobre el rendimiento del modelo.\n",
        "\n",
        "Este código cubre un ciclo completo de trabajo con perceptrones, desde la implementación más básica hasta el uso de modelos optimizados y técnicas avanzadas en scikit-learn. Es un excelente ejercicio para entender cómo funcionan los perceptrones y cómo se pueden evaluar y ajustar los modelos de clasificación binaria.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BAmn_bbY_-qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importar librerias y Preparación de datos"
      ],
      "metadata": {
        "id": "4Bmj-fsUXu52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "CoshbDo0YAeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIg7eGg2_5fm"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 1. Preparación de los Datos\n",
        "# ======================\n",
        "# Datos de estudiantes representados como [notas_IA, PGA], con valores normalizados entre 0 y 1.\n",
        "alumnos = np.array([[0.3, 0.4], [0.4, 0.3],\n",
        "                     [0.3, 0.2], [0.4, 0.1],\n",
        "                     [0.5, 0.2], [0.4, 0.8],\n",
        "                     [0.6, 0.8], [0.5, 0.6],\n",
        "                     [0.7, 0.6], [0.8, 0.5]])\n",
        "\n",
        "# Etiquetas de clase: 1 = Se gradúa, 0 = Se retira\n",
        "clases = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combinar los datos y las clases para facilitar la visualización\n",
        "data = np.column_stack((alumnos, clases))\n",
        "\n",
        "# Crear un DataFrame para análisis\n",
        "columnas = ['Nota IA', 'GPA', 'Estado']\n",
        "df = pd.DataFrame(data, columns=columnas)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "--R1eN9X66M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3aT2r2CJq_KK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Verificar si son linealmente separables\n",
        "\n",
        "En la gráfica puede apreciarse si a través de una línea recta se pueden separar los estudiantes que se gradúan o se retiran"
      ],
      "metadata": {
        "id": "gWUfwzMtYS0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ======================\n",
        "# 2. Visualización de Datos\n",
        "# ======================\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.title(\"¿Éxito en Ingeniería?\", fontsize=20)\n",
        "plt.scatter(alumnos[clases == 0].T[0],\n",
        "            alumnos[clases == 0].T[1],\n",
        "            marker=\"x\", s=180, color=\"red\",\n",
        "            linewidths=5, label=\"Se retira\")\n",
        "plt.scatter(alumnos[clases == 1].T[0],\n",
        "            alumnos[clases == 1].T[1],\n",
        "            marker=\"o\", s=180, color=\"blue\",\n",
        "            linewidths=5, label=\"Se gradúa\")\n",
        "plt.xlabel(\"Nota IA\", fontsize=15)\n",
        "plt.ylabel(\"PGA\", fontsize=15)\n",
        "plt.legend(bbox_to_anchor=(1.3, 0.15))\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SUDZSWh7AlvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementación del perceptrón manualmente\n",
        "##Definición de la función de activación"
      ],
      "metadata": {
        "id": "zPwwUzknA_Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 3. Implementación del Perceptrón\n",
        "# ======================\n",
        "# Función de activación (Escalón): Decide la clase basándose en la suma ponderada\n",
        "\n",
        "def activacion(pesos, x, b):\n",
        "    z = np.dot(pesos, x)\n",
        "    return 1 if z + b > 0 else 0\n",
        "\n",
        "#Recordar que z es la función suma y b es el bias."
      ],
      "metadata": {
        "id": "QM8PotpCBBR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align}\n",
        "         & z= w_1 x_1 + w_2 x_2+b\n",
        "\\end{align}\n",
        "\n",
        "![imagen](https://github.com/adiacla/bigdata/blob/master/FuncionEscalon.png?raw=true)"
      ],
      "metadata": {
        "id": "dMvwWaIh_TqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer una semilla para reproducibilidad\n",
        "np.random.seed(42)  # Puedes cambiar 42 por cualquier número entero\n",
        "\n",
        "# Inicializar pesos y bias aleatorios\n",
        "pesos = np.random.uniform(-1, 1, size=2)\n",
        "b = np.random.uniform(-1, 1)\n",
        "print(\"Pesos W1:\", pesos[0], \"   Pesos W2:\", pesos[1])\n",
        "print(\"Bias o umbral:\", b)\n",
        "#El aleatorio puede variar si cambia la semilla."
      ],
      "metadata": {
        "id": "XtS2k1mc-Dqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![imagen](https://github.com/adiacla/bigdata/blob/master/pesosybias.png?raw=true)"
      ],
      "metadata": {
        "id": "4UQ9qj5FuXuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predicciones de pruebas\n",
        "tenemos dos alumnos nuevos que tienen estos datosen las notas de IA y GPA (calificiacón IA y el Promedio General Acumulado)\n",
        "\n",
        "a. [0.4, 0.3]\n",
        "b. [0.8, 0.9]\n",
        "\n",
        "Hagamos predicciones"
      ],
      "metadata": {
        "id": "Y4tJPFK2vFe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar la activación con datos de ejemplo\n",
        "print(f\"Predicción para [0.4, 0.3]: {activacion(pesos, [0.4, 0.3], b)}\")\n",
        "print(f\"Predicción para [0.8, 0.9]: {activacion(pesos, [0.8, 0.9], b)}\")"
      ],
      "metadata": {
        "id": "5Ltrk6ThC8dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asi acabamos de hacer un pequeño perceptrón, pero es claro que el valor de w1,w2 y b fueron aleatorios y no están optimizados, ahora debemos entrenar.\n",
        " por lo tanto la predicción **es errada.**\n",
        "\n",
        "\n",
        "Vamos a optimizar a partir del error de activación."
      ],
      "metadata": {
        "id": "471G3dUqCxMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pseudocódigo del entrenamiento del Perceptrón\n",
        "\n",
        "Haremos un ciclo para ir cambiando los datos de los pesos y el umbral, hasta tener el menor error aceptable (por ejemplo si es cero) o se terminen las epocas.\n",
        "\n",
        "La tasa de aprendizaje es la tasa con la que se va actualizar los pesos, para que se vayan ajustando hasta que lleguen a valor mínimo de error, debemos probar con varias tasas.\n",
        "\n",
        "##Algortimo\n",
        "\n",
        "0. Inicializar los pesos y el umbral\n",
        "1. inicia epocas máximas = 100 (Se puede modificar)\n",
        "2. inicia epoca = 0\n",
        "3. inicia tasa de aprendizaje = 0.01 (Se puede modificar)\n",
        "4. Mientras (epoca < epocas máximas) hacer:\n",
        "5.    Para cada instancia (o registro) de entrenamiento hacer:\n",
        "          Calcula salida del perceptrón para esa estancia\n",
        "          Calcula el error\n",
        "          Actualiza pesos y umbral usando la tasa de aprendizaje, la instancia y el error\n",
        "9.    epoca += 1"
      ],
      "metadata": {
        "id": "iuwIq-ijBHTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 4. Entrenamiento del Perceptrón\n",
        "# ======================\n",
        "# Inicialización de parámetros\n",
        "pesos = np.random.uniform(-1, 1, size=2)\n",
        "b = np.random.uniform(-1, 1)\n",
        "epocas = 100\n",
        "tasa_de_aprendizaje = 0.01\n",
        "\n",
        "# Ciclo de entrenamiento\n",
        "for epoca in range(epocas):\n",
        "    error_total = 0\n",
        "\n",
        "    for i in range(len(alumnos)):\n",
        "        # Realizar una predicción\n",
        "        prediccion = activacion(pesos, alumnos[i], b)\n",
        "\n",
        "        # Calcular el error\n",
        "        error = clases[i] - prediccion\n",
        "        error_total += error ** 2\n",
        "\n",
        "        # Actualizar los pesos y el bias\n",
        "        pesos += tasa_de_aprendizaje * error * alumnos[i]\n",
        "        b += tasa_de_aprendizaje * error\n",
        "\n",
        "    # Imprimir el error total por época\n",
        "    print(f\"Época {epoca + 1}: Error total = {error_total}\")\n"
      ],
      "metadata": {
        "id": "Q3IsMHTxBMCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cómo se puede obervar desde la época 65 ya no tenemos errores, por lo tanto el modelo \"convergio\" a 0 errores en total a partir de ajustar los pesos y los bias."
      ],
      "metadata": {
        "id": "1vgp95X5w0ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Época en el Entrenamiento de un Modelo\n",
        "En el contexto del aprendizaje automático, una época se refiere a una iteración completa sobre el conjunto de datos de entrenamiento. Durante una época, el modelo:\n",
        "\n",
        "* Toma cada instancia del conjunto de datos de entrenamiento.\n",
        "\n",
        "* Calcula predicciones utilizando la función de activación.\n",
        "\n",
        "* Actualiza los pesos y el umbral (o bias) en función del error cometido.\n",
        "\n",
        "Un modelo puede necesitar varias épocas para que sus parámetros (pesos y umbral) se ajusten adecuadamente y minimicen el error de predicción.\n",
        "\n",
        "##Cantidad de épocas necesarias\n",
        "\n",
        "**Pocas épocas:** El modelo puede no aprender lo suficiente, resultando en un modelo subentrenado (underfitting).\n",
        "\n",
        "**Demasiadas épocas:** El modelo puede aprender detalles específicos del conjunto de datos, lo que lleva a sobreentrenamiento (overfitting).\n",
        "\n",
        "\n",
        "***Regla general:*** Comienza con un número moderado, como 100 o 500, y evalúa el desempeño. Si el error sigue disminuyendo, podrías aumentar las épocas.\n",
        "\n"
      ],
      "metadata": {
        "id": "C6f9N0thyugw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tasa de Aprendizaje\n",
        "La tasa de aprendizaje (𝜂) es un parámetro que controla la magnitud de los ajustes realizados a los pesos y al umbral después de cada predicción.\n",
        "\n",
        "Es un número pequeño, típicamente en el rango de 0.001  a\n",
        "0.1.\n",
        "Determina qué tan rápido o lento aprende el modelo.\n",
        "\n",
        "Fórmula de actualización de los pesos:\n",
        "$$𝑤_{𝑛𝑢𝑒𝑣𝑜=𝑤𝑎𝑐𝑡𝑢𝑎𝑙}+𝜂⋅error⋅entrada$$\n",
        "\n",
        "**Impacto del valor de la tasa de aprendizaje:**Tasa de aprendizaje muy alta (𝜂=0.5 o más):\n",
        "\n",
        "El modelo puede ajustar demasiado rápido, dando lugar a oscilaciones alrededor de los valores ideales.\n",
        "\n",
        "Podría no converger a una solución estable.\n",
        "\n",
        "**Tasa de aprendizaje muy baja** (𝜂<=0.0001):\n",
        "\n",
        "El modelo aprenderá muy lentamente.\n",
        "Puede necesitar muchas épocas para alcanzar un error aceptable.\n",
        "\n",
        "\n",
        "***Regla general:** Comienza con una tasa de aprendizaje entre\n",
        "0.01 y 0.1.\n",
        "\n",
        "***Ajusta según el comportamiento del modelo:**\n",
        "\n",
        "Si el error no disminuye lo suficiente, incrementa ligeramente.\n",
        "\n",
        "Si el error oscila o no converge, reduce el valor.\n",
        "\n",
        "\n",
        "**Visualización del error:**\n",
        "\n",
        "Traza una gráfica del error total frente al número de épocas.\n",
        "Si el error disminuye de manera consistente y se estabiliza, los valores pueden ser adecuados.\n",
        "\n",
        "Si el error oscila o se detiene prematuramente, ajusta la tasa de aprendizaje o incrementa las épocas.\n",
        "![imagen](https://github.com/adiacla/bigdata/blob/master/epochs.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "**Validación cruzada:**\n",
        "\n",
        "* Divide los datos en entrenamiento y validación.\n",
        "* Ajusta la tasa de aprendizaje y el número de épocas para minimizar el error en el conjunto de validación, evitando el sobreentrenamiento.\n",
        "\n",
        "**Uso de técnicas avanzadas:**\n",
        "\n",
        "Implementa un programa que disminuya automáticamente la tasa de aprendizaje durante el entrenamiento si el modelo no mejora (decay learning rate).\n",
        "\n",
        "Usa herramientas como grid search o random search para encontrar los mejores hiperparámetros.\n",
        "\n",
        "**Analogía para Facilitar la Comprensión**\n",
        "\n",
        "Piensa en la tasa de aprendizaje como el tamaño de los pasos de una persona y en las épocas como el número total de pasos en una caminata:\n",
        "\n",
        "* Tasa de aprendizaje grande: Das zancadas largas, pero podrías pasar de largo el destino.\n",
        "* Tasa de aprendizaje pequeña: Das pasos cortos, pero llegarás al destino más lentamente.\n",
        "\n",
        "Épocas: Representan cuántos pasos estás dispuesto a dar para llegar a tu objetivo.\n",
        "\n",
        "El equilibrio ideal depende del terreno (tus datos) y la distancia al destino (minimizar el error).\n",
        "\n"
      ],
      "metadata": {
        "id": "Njo1Mdj_znWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predecir el con perceptrón entrenado\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-EriWj9Xw-by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar la activación con datos de ejemplo\n",
        "print(f\"Predicción para [0.4, 0.3]: {activacion(pesos, [0.4, 0.3], b)}\")\n",
        "print(f\"Predicción para [0.8, 0.9]: {activacion(pesos, [0.8, 0.9], b)}\")"
      ],
      "metadata": {
        "id": "0eUcGMyt7A2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En conclusión vemos cómo ahora la predicción para el alumno con las notas: [0.4, 0.3] es 0 es decir que se retira. Contratio al alumno [0.8, 0.9]: 1, que se gradua."
      ],
      "metadata": {
        "id": "sxaia6cTxc22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graficar las Zonas de aprobación\n",
        "\n",
        "En la siguiete gráfica vemos cómo el modelo separa los que se graduan y los que se retirar, en una gráfica 2D, dados que es fácil apreciarlos porque tenemos solo dos carcteristicas (Notas IA y PGA). En el fondo de la gráfica notará unos puntos azules y otros rojos. Esas son las zonas de separación."
      ],
      "metadata": {
        "id": "JCcvObQIBT4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 5. Visualización de la Clasificación\n",
        "# ======================\n",
        "plt.figure(figsize=(6, 5), dpi=200)\n",
        "plt.title(\"¿Éxito en Ingeniería?\", fontsize=20)\n",
        "\n",
        "# Graficar los puntos de datos\n",
        "plt.scatter(alumnos[clases == 0].T[0],\n",
        "            alumnos[clases == 0].T[1],\n",
        "            marker=\"x\", s=180, color=\"red\",\n",
        "            linewidths=5, label=\"Se retira\")\n",
        "plt.scatter(alumnos[clases == 1].T[0],\n",
        "            alumnos[clases == 1].T[1],\n",
        "            marker=\"o\", s=180, color=\"blue\",\n",
        "            linewidths=5, label=\"Se gradúa\")\n",
        "\n",
        "# Graficar las regiones de decisión\n",
        "for nota in np.arange(0, 1, 0.05):\n",
        "    for GPA in np.arange(0, 1, 0.05):\n",
        "        color = activacion(pesos, [nota, GPA], b)\n",
        "        plt.scatter(nota, GPA, marker=\"s\", s=10,\n",
        "                    color=\"blue\" if color == 1 else \"red\",\n",
        "                    alpha=0.2, linewidths=0)\n",
        "\n",
        "plt.xlabel(\"Nota IA\", fontsize=15)\n",
        "plt.ylabel(\"PGA\", fontsize=15)\n",
        "plt.legend(bbox_to_anchor=(1.3, 0.15))\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xvnlPrZMBEj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptrón con Scikit-learn\n",
        "\n",
        "El objetivo es replicar la lógica de un perceptrón manual, pero utilizando la implementación optimizada que proporciona la biblioteca scikit-learn. Esto nos permitirá realizar tareas similares con menos esfuerzo, mayor optimización, y acceso a métricas adicionales.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jagm6BoIDJeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación de los pasos\n",
        "1. Importación de bibliotecas\n",
        "Primero, importamos las bibliotecas necesarias:\n",
        "\n",
        "**Perceptron de sklearn.linear_model:** Clase que implementa el algoritmo del perceptrón.\n",
        "\n",
        "**numpy:** Para trabajar con los datos en forma de matrices.\n",
        "\n"
      ],
      "metadata": {
        "id": "oHln7SyY42QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7MGMP8n55B4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Datos de entrenamiento\n",
        "Usamos los mismos datos de entrenamiento del ejemplo anterior. Estos datos contienen:\n",
        "\n",
        "X_train: Las características de los alumnos (notas_IA y PGA).\n",
        "y_train: Las etiquetas de clase (0: \"Se retira\", 1: \"Se gradúa\")."
      ],
      "metadata": {
        "id": "ddBj3bA75Fp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos de entrenamiento\n",
        "X_train = alumnos  # Características\n",
        "y_train = clases   # Etiquetas"
      ],
      "metadata": {
        "id": "-xqyfjlA5Qom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creación del modelo\n",
        "Creamos una instancia de la clase Perceptron.\n",
        "\n",
        "Algunos parámetros importantes:\n",
        "\n",
        "verbose=True: Muestra información sobre el progreso del entrenamiento.\n",
        "\n",
        "Otros parámetros opcionales (no utilizados aquí):\n",
        "\n",
        "max_iter: Número máximo de épocas. Por defecto,\n",
        "1000.\n",
        "\n",
        "eta0: Tasa de aprendizaje inicial. Por defecto,\n",
        "1.0"
      ],
      "metadata": {
        "id": "3JkgkQ_D5TXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo de Perceptrón\n",
        "perceptron = Perceptron(verbose=True)"
      ],
      "metadata": {
        "id": "kwjWaxqz5gAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Entrenamiento del modelo\n",
        "\n",
        "El método fit ajusta el modelo al conjunto de datos de entrenamiento. Internamente:\n",
        "\n",
        "* Inicializa pesos y bias.\n",
        "\n",
        "* Itera por los datos, ajustando los pesos en función del error."
      ],
      "metadata": {
        "id": "GrKGmiEm5nXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo\n",
        "perceptron.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "0g4PLTrE5vCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Realización de predicciones\n",
        "\n",
        "El método predict se usa para predecir la clase de nuevos datos. Aquí realizamos predicciones para dos instancias nuevas:\n",
        "\n",
        "[0.2,0.2]: Características bajas.\n",
        "\n",
        "[0.8,0.8]: Características altas"
      ],
      "metadata": {
        "id": "e9Zp_uGe5979"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones con datos nuevos\n",
        "predictions = perceptron.predict([[0.2, 0.2], [0.8, 0.8]])\n",
        "print(\"Predicciones:\", predictions)\n"
      ],
      "metadata": {
        "id": "REm_A_Mp6Gub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inspección del modelo entrenado\n",
        "\n",
        "Después del entrenamiento, se pueden consultar varias propiedades del modelo:\n",
        "\n",
        "* n_iter_: Número de iteraciones realizadas (épocas).\n",
        "coef_: Pesos aprendidos (𝑤1 ,𝑤2, etc.).\n",
        "\n",
        "* intercept_: El bias o umbral.\n",
        "\n",
        "* n_features_in_: Número de características usadas en el entrenamiento.\n"
      ],
      "metadata": {
        "id": "2KceudO66L0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron"
      ],
      "metadata": {
        "id": "10sJdpl7VCa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consultar información del modelo\n",
        "print(\"Épocas realizadas:\", perceptron.n_iter_)\n",
        "print(\"Pesos aprendidos:\", perceptron.coef_)\n",
        "print(\"El bias es:\", perceptron.intercept_)\n",
        "print(\"Número de características utilizadas:\", perceptron.n_features_in_)\n"
      ],
      "metadata": {
        "id": "8mL1RjiCVez1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#permite saber las clases que predice\n",
        "perceptron.classes_"
      ],
      "metadata": {
        "id": "AFN_9ecfVkKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ajustando parámetros\n",
        "\n",
        "1. learning_rate (Tasa de aprendizaje)\n",
        "\n",
        "En el Perceptrón de sklearn, no configuramos directamente el learning rate, pero se controla implícitamente a través de la tasa inicial (𝜂0 ) y del algoritmo de optimización.\n",
        "\n",
        "En el perceptrón estándar, esta tasa define cuánto se ajustan los pesos en cada iteración en función del error.\n",
        "\n",
        "2. max_iter (Número máximo de iteraciones o épocas)\n",
        "\n",
        "Especifica el número máximo de iteraciones sobre los datos de entrenamiento. Si el modelo converge antes, el entrenamiento puede detenerse prematuramente.\n",
        "\n",
        "**Ajustar este parámetro es clave:**\n",
        "\n",
        "* Valores bajos: Entrenamiento rápido pero puede no alcanzar la convergencia.\n",
        "* Valores altos: Entrenamiento más largo con mayores posibilidades de convergencia.\n",
        "\n",
        "3. tol (Tolerancia)\n",
        "\n",
        "Es el valor límite que define si el algoritmo ha convergido.\n",
        "Cuando la reducción del error entre épocas sucesivas es menor que este valor, el entrenamiento se detiene.\n",
        "\n",
        "**Valores comunes:**\n",
        "\n",
        "* tol=1e-3 (0.001): Tolerancia estándar.\n",
        "* Valores menores (ej., 1e-5) permiten más precisión pero aumentan el tiempo de entrenamiento.\n",
        "\n",
        "4. random_state (Semilla aleatoria)\n",
        "\n",
        "Determina el estado inicial del generador de números aleatorios utilizado para inicializar pesos.\n",
        "\n",
        "**Es útil para:**\n",
        "\n",
        "* Reproducibilidad: Obtener los mismos resultados en ejecuciones diferentes.\n",
        "* Pruebas controladas: Comparar el desempeño con diferentes configuraciones."
      ],
      "metadata": {
        "id": "7MCZqkWL64kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Early Stopping</h3>\n",
        "<ul>\n",
        "<li><strong>Definici&oacute;n</strong>: El <code>early stopping</code> es una t&eacute;cnica que detiene el entrenamiento de un modelo cuando el rendimiento en un conjunto de validaci&oacute;n comienza a empeorar. Esto se hace para prevenir el sobreajuste, es decir, cuando el modelo empieza a aprender el ruido en lugar de las verdaderas relaciones en los datos.</li>\n",
        "<li><strong>Uso</strong>: Se suele utilizar en el entrenamiento de redes neuronales y otros modelos que se entrenan durante muchas &eacute;pocas. Se monitorea el rendimiento en un conjunto de validaci&oacute;n y se detiene el entrenamiento si no hay mejora durante un n&uacute;mero espec&iacute;fico de &eacute;pocas (llamado \"patience\").</li>\n",
        "<li><strong>Objetivo</strong>: Asegurarse de que el modelo generalice bien a datos no vistos, evitando que se sobreajuste a los datos de entrenamiento.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "e3hEV9YR8wYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(clases),len(alumnos)"
      ],
      "metadata": {
        "id": "EO6BPjb05iZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "import numpy as np\n",
        "\n",
        "# Datos de entrenamiento\n",
        "X_train = alumnos  # Características: [notas_IA, PGA]\n",
        "y_train = clases   # Etiquetas: [0 (Se retira), 1 (Se gradúa)]\n",
        "\n",
        "# Crear el modelo del Perceptrón\n",
        "# max_iter=500: Permitir hasta 500 iteraciones (épocas)\n",
        "# tol=1e-3: Detener si el cambio en el error es menor a 0.001\n",
        "# verbose=1: Mostrar información del entrenamiento\n",
        "# random_state=123: Fijar la semilla para reproducibilidad\n",
        "perceptron = Perceptron(max_iter=500, tol=1e-3, verbose=1, random_state=123,early_stopping=False)\n",
        "\n",
        "# Entrenar el modelo\n",
        "perceptron.fit(X_train, y_train)\n",
        "print(\"-\"*80,\"\\n\")\n",
        "# Hacer predicciones\n",
        "# Ejemplo: Predecir las clases para las características [0.2, 0.2] y [0.8, 0.8]\n",
        "predictions = perceptron.predict([[0.2, 0.2], [0.8, 0.8]])\n",
        "print(\"Predicciones:\", predictions)\n",
        "\n",
        "print(\"-\"*80,\"\\n\")\n",
        "# Inspeccionar el modelo entrenado\n",
        "print(\"Pesos aprendidos (coeficientes):\", perceptron.coef_)\n",
        "print(\"Bias aprendido:\", perceptron.intercept_)\n",
        "print(\"Número de iteraciones realizadas:\", perceptron.n_iter_)\n"
      ],
      "metadata": {
        "id": "182cB2bvWG9r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}